{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">The goal here is to perform sentiment analysis using distilbert. (https://medium.com/huggingface/distilbert-8cf3380435b5)\n",
    " \n",
    "We will encode sentences with distillbert,then feed these as feature to a xgboost classifier.\n",
    "\n",
    "We will be using the Stanford Sentiment Treebank 2 (SST2) dataset (Movie reviews with one sentence per review, labeled either positive:1, or negative: 0, no neutral)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset on a pandas dataframe, we don't get the whole set , only the train part\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room floor of any given daytime soap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love , memory , history and the war between art and commerce</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been the be all end all of the modern office anomie films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                 0  \\\n",
       "0  a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films                                                                                                                           \n",
       "1  apparently reassembled from the cutting room floor of any given daytime soap                                                                                                                                                      \n",
       "2  they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes   \n",
       "3  this is a visually stunning rumination on love , memory , history and the war between art and commerce                                                                                                                            \n",
       "4  jonathan parker 's bartleby should have been the be all end all of the modern office anomie films                                                                                                                                 \n",
       "\n",
       "   1  \n",
       "0  1  \n",
       "1  0  \n",
       "2  0  \n",
       "3  1  \n",
       "4  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See what the dataset looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Let's load the pretrained distilBert model and its tokenizer, with the \"transformers\" library from huggingface, we won't be training or fine tuning the model for now, just use it for inference. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert=TFDistilBertModel.from_pretrained('distilbert-base-uncased')#load distilbert,a lot of gpu memory required\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")#load the tokenizer for distilbert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "#Tokenize each sentence of our dataset,\n",
    "#the first token is always <CLS>,it stands for classification,it's what we will retrieve at the end.\n",
    "\n",
    "padded=pad_sequences(tokenized,padding='post')\n",
    "#we pad our inputs becasuse bert need to have all inputs the same length \n",
    "#bert does better with padding on the right rather than on the left hence post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Bert also need attention masks since inputs are padded</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0) #create the attention masks\n",
    "attention_mask.shape #6920 inputs, each having a length of 67 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_splits=np.split(padded,8) # We split the inputs,otherwise my gpu runs out of memory during inference\n",
    "mask_splits=np.split(attention_mask,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128,\n",
       "       16603,  1997,  5053,  1998,  1996,  6841,  1998,  5687,  5469,\n",
       "        3152,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_splits[0][0] #exemple of an tokenized sentence ,101 is the CLS token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run inference on our tokenized dataset\n",
    "outputs=[]\n",
    "for i in range(len(padded_splits)):\n",
    " outputs.append(distil_bert(padded_splits[i],attention_mask=mask_splits[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del distil_bert\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We concatenate the last hidden states,into one numpy array\n",
    "last_hidden=np.concatenate(( outputs[0][0].numpy(),outputs[1][0].numpy()\n",
    "                            ,outputs[2][0].numpy(),outputs[3][0].numpy()\n",
    "                            ,outputs[4][0].numpy(),outputs[5][0].numpy()\n",
    "                            ,outputs[6][0].numpy(),outputs[7][0].numpy()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the features to disk \n",
    "np.save('/home/florian/bert/last_hidden_uni.npy',last_hidden)\n",
    "# need to restart the kernel to clear gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the feature from disk \n",
    "last_hidden=np.load('/home/florian/bert/last_hidden_uni.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.shape\n",
    "#(number of sentences,embedding length ,number of units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden[:,0,:] #We only want to retreive the first token (cls) of each units\n",
    "\n",
    "targets=df[1] #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 768)\n",
      "(6920,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593428, -0.14028901,  0.008311  , ..., -0.13694826,\n",
       "         0.5867007 ,  0.20112711],\n",
       "       [-0.17262739, -0.14476174,  0.00223407, ..., -0.17442545,\n",
       "         0.21386476,  0.37197474],\n",
       "       [-0.05063341,  0.07203925, -0.02959689, ..., -0.0714896 ,\n",
       "         0.7185235 ,  0.26225457],\n",
       "       ...,\n",
       "       [-0.06550973, -0.05184762, -0.14094462, ..., -0.06450678,\n",
       "         0.60223097,  0.2134794 ],\n",
       "       [-0.08523114, -0.04869815, -0.08137507, ..., -0.13589332,\n",
       "         0.39505604,  0.22889736],\n",
       "       [-0.29436877, -0.09234713, -0.00831658, ..., -0.05159125,\n",
       "         0.43497816,  0.2889163 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Each sentence in the dataset is now embedded as a vector of 768 features, that we can feed to an Xgboost classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "Y = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic',silent=True,early_stopping_rounds=200,tree_method='gpu_hist')\n",
    "params = {\n",
    "        'n_estimators': stats.randint(1500, 4000),\n",
    "        'learning_rate': [0.03,0.04,0.05,0.06],\n",
    "        'min_child_weight': [3,4,5,6],\n",
    "        'gamma': [2,3,4,5,6],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> We use random search and stratified cross validation to hypertune xgboost ,with accuracy as our metric </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5 # 5 fold cross validation\n",
    "param_comb = 5 # 5 iteration per fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=param_comb, \n",
    "                                   scoring='accuracy', n_jobs=1, cv=skf.split(X,Y), verbose=3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6, score=0.832, total=  26.5s\n",
      "[CV] colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6, score=0.843, total=  26.2s\n",
      "[CV] colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   52.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6, score=0.846, total=  26.4s\n",
      "[CV] colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6, score=0.847, total=  26.0s\n",
      "[CV] colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=5, learning_rate=0.03, max_depth=5, min_child_weight=5, n_estimators=2595, subsample=0.6, score=0.857, total=  26.1s\n",
      "[CV] colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6, score=0.838, total=  24.4s\n",
      "[CV] colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6, score=0.842, total=  24.4s\n",
      "[CV] colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6, score=0.847, total=  24.5s\n",
      "[CV] colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6, score=0.841, total=  25.5s\n",
      "[CV] colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=3, learning_rate=0.05, max_depth=5, min_child_weight=5, n_estimators=2982, subsample=0.6, score=0.853, total=  26.7s\n",
      "[CV] colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6, score=0.841, total=  35.3s\n",
      "[CV] colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6, score=0.842, total=  35.2s\n",
      "[CV] colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6, score=0.844, total=  34.9s\n",
      "[CV] colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6, score=0.848, total=  34.6s\n",
      "[CV] colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6 \n",
      "[CV]  colsample_bytree=1.0, gamma=6, learning_rate=0.04, max_depth=4, min_child_weight=4, n_estimators=3933, subsample=0.6, score=0.852, total=  35.3s\n",
      "[CV] colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6, score=0.835, total=  19.5s\n",
      "[CV] colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6, score=0.847, total=  19.3s\n",
      "[CV] colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6, score=0.845, total=  18.9s\n",
      "[CV] colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6, score=0.840, total=  19.0s\n",
      "[CV] colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6 \n",
      "[CV]  colsample_bytree=0.6, gamma=5, learning_rate=0.04, max_depth=4, min_child_weight=3, n_estimators=2247, subsample=0.6, score=0.854, total=  19.3s\n",
      "[CV] colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0 \n",
      "[CV]  colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0, score=0.835, total=  31.0s\n",
      "[CV] colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0 \n",
      "[CV]  colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0, score=0.837, total=  31.1s\n",
      "[CV] colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0 \n",
      "[CV]  colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0, score=0.843, total=  30.1s\n",
      "[CV] colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0 \n",
      "[CV]  colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0, score=0.842, total=  31.0s\n",
      "[CV] colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0 \n",
      "[CV]  colsample_bytree=0.6, gamma=4, learning_rate=0.05, max_depth=5, min_child_weight=4, n_estimators=3547, subsample=1.0, score=0.853, total=  31.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 722.1062324047089\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "random_search.fit(X, Y)\n",
    "end_time=time.time()-start_time\n",
    "print(\"time:\" ,end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1.0,\n",
      "              early_stopping_rounds=200, gamma=6, learning_rate=0.04,\n",
      "              max_delta_step=0, max_depth=4, min_child_weight=4, missing=None,\n",
      "              n_estimators=3933, n_jobs=1, nthread=None,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
      "              subsample=0.6, tree_method='gpu_hist', verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8453757225433526\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy :\",random_search.best_score_)\n",
    "#0.845 accuracy without training or fine-tuning distilbert on this specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
